4
/home/wxt/.conda/envs/halos3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Making experiment directory /home/wxt/.cache/huggingface/hub/llama2_7b_ppo_halos_2
no FSDP port specified; using open port for FSDP: 45883
seed: 1
exp_name: llama2_7b_ppo_halos_2
datasets:
- hh
mode: train
debug: false
use_fsdp: true
fsdp_port: 45883
wandb:
  enabled: true
  entity: null
  project: archangel
cache_dir: /home/wxt/.cache/huggingface/hub
local_run_dir: /home/wxt/.cache/huggingface/hub/llama2_7b_ppo_halos_2
do_first_eval: true
minimum_log_interval_secs: 1.0
intermediate_checkpoints: false
trainer: PPOTrainer
lr: 1.0e-06
n_epochs: 1
n_examples: null
optimizer: RMSprop
warmup_steps: 150
eval_every: 4000
n_samples: 128
samples_dir: samples/
n_eval_examples: 512
saved_policy: /home/wxt/.cache/huggingface/hub/llama2_7b_ppo_halos_2/LATEST/policy.pt
top_p: 0.95
human_prefix: '

  <|user|>

  '
assistant_prefix: '

  <|assistant|>

  '
human_suffix: ''
assistant_suffix: ''
frac_unique_desirable: 1.0
frac_unique_undesirable: 1.0
model:
  name_or_path: daryl149/llama-2-7b-hf
  tokenizer_name_or_path: null
  load_from: /home/wxt/.cache/huggingface/hub/llama2_7b_sft_halos_2_3/LATEST/policy.pt
  block_name: LlamaDecoderLayer
  policy_dtype: bfloat16
  fsdp_policy_mp: null
  reference_dtype: bfloat16
  max_grad_norm: 10.0
  v_head_max_grad_norm: 0.1
  max_length: 1024
  max_prompt_length: 512
  activation_checkpointing: true
  batch_size: 16
  gradient_accumulation_steps: 4
  eval_batch_size: 16
  use_flash_attention: false
loss:
  name: ppo
  ppo_epochs: 1
  cliprange: 0.5
  trainer: PPOTrainer
  dataloader: UnpairedPreferenceDataLoader
  lam: 0.95
  gamma: 0.99
  critic_coef: 0.01
  KL_coef: 0.1
  use_reference_model: true

================================================================================
Writing to design-agent-09:/home/wxt/.cache/huggingface/hub/llama2_7b_ppo_halos_2
================================================================================
building policy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]
building reference model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.93s/it]
loading pre-trained weights at step 159968 from /home/wxt/.cache/huggingface/hub/llama2_7b_sft_halos_2_3/LATEST/policy.pt with metrics {}
loaded pre-trained weights
Loading tokenizer daryl149/llama-2-7b-hf
0 special tokens added
Loading HH dataset (train split) from Huggingface...
Processing HH:   0%|          | 0/160800 [00:00<?, ?it/s]Processing HH:   1%|          | 1492/160800 [00:00<00:10, 14914.61it/s]Processing HH:   2%|▏         | 2984/160800 [00:00<00:10, 14837.99it/s]Processing HH:   3%|▎         | 4516/160800 [00:00<00:10, 15054.09it/s]Processing HH:   4%|▍         | 6035/160800 [00:00<00:10, 15106.67it/s]Processing HH:   5%|▍         | 7546/160800 [00:00<00:10, 14936.50it/s]Processing HH:   6%|▌         | 9040/160800 [00:00<00:10, 14828.50it/s]Processing HH:   7%|▋         | 10524/160800 [00:00<00:10, 14682.04it/s]Processing HH:   7%|▋         | 11993/160800 [00:00<00:10, 14663.17it/s]Processing HH:   8%|▊         | 13506/160800 [00:00<00:09, 14805.28it/s]Processing HH:   9%|▉         | 14987/160800 [00:01<00:14, 10216.33it/s]Processing HH:  10%|█         | 16483/160800 [00:01<00:12, 11313.27it/s]Processing HH:  11%|█         | 17985/160800 [00:01<00:11, 12233.17it/s]Processing HH:  12%|█▏        | 19440/160800 [00:01<00:11, 12838.69it/s]Processing HH:  13%|█▎        | 20952/160800 [00:01<00:10, 13459.53it/s]Processing HH:  14%|█▍        | 22458/160800 [00:01<00:09, 13906.51it/s]Processing HH:  15%|█▍        | 23922/160800 [00:01<00:09, 14115.80it/s]Processing HH:  16%|█▌        | 25376/160800 [00:01<00:09, 14113.94it/s]Processing HH:  17%|█▋        | 26886/160800 [00:01<00:09, 14398.25it/s]Processing HH:  18%|█▊        | 28357/160800 [00:02<00:09, 14489.39it/s]Processing HH:  19%|█▊        | 29849/160800 [00:02<00:08, 14615.38it/s]Processing HH:  19%|█▉        | 31322/160800 [00:02<00:08, 14624.43it/s]Processing HH:  20%|██        | 32800/160800 [00:02<00:08, 14663.60it/s]Processing HH:  21%|██▏       | 34272/160800 [00:02<00:12, 10466.66it/s]Processing HH:  22%|██▏       | 35799/160800 [00:02<00:10, 11583.62it/s]Processing HH:  23%|██▎       | 37216/160800 [00:02<00:10, 12228.95it/s]Processing HH:  24%|██▍       | 38742/160800 [00:02<00:09, 13023.58it/s]Processing HH:  25%|██▌       | 40303/160800 [00:02<00:08, 13727.60it/s]Processing HH:  26%|██▌       | 41826/160800 [00:03<00:08, 14147.35it/s]Processing HH:  27%|██▋       | 43295/160800 [00:03<00:08, 14270.04it/s]Processing HH:  28%|██▊       | 44761/160800 [00:03<00:08, 14272.27it/s]Processing HH:  29%|██▊       | 46217/160800 [00:03<00:07, 14354.90it/s]Processing HH:  30%|██▉       | 47672/160800 [00:03<00:07, 14236.87it/s]Processing HH:  31%|███       | 49110/160800 [00:03<00:07, 14177.28it/s]Processing HH:  31%|███▏      | 50538/160800 [00:03<00:07, 14131.74it/s]Processing HH:  32%|███▏      | 51958/160800 [00:03<00:07, 13863.67it/s]Processing HH:  33%|███▎      | 53399/160800 [00:03<00:07, 14022.34it/s]Processing HH:  34%|███▍      | 54840/160800 [00:04<00:07, 14133.78it/s]Processing HH:  35%|███▍      | 56257/160800 [00:04<00:07, 13949.98it/s]Processing HH:  36%|███▌      | 57655/160800 [00:04<00:10, 9799.51it/s] Processing HH:  37%|███▋      | 59076/160800 [00:04<00:09, 10803.05it/s]Processing HH:  38%|███▊      | 60444/160800 [00:04<00:08, 11507.81it/s]Processing HH:  38%|███▊      | 61854/160800 [00:04<00:08, 12180.04it/s]Processing HH:  39%|███▉      | 63225/160800 [00:04<00:07, 12592.02it/s]Processing HH:  40%|████      | 64645/160800 [00:04<00:07, 13038.62it/s]Processing HH:  41%|████      | 66065/160800 [00:04<00:07, 13369.05it/s]Processing HH:  42%|████▏     | 67441/160800 [00:05<00:06, 13436.69it/s]Processing HH:  43%|████▎     | 68862/160800 [00:05<00:06, 13661.85it/s]Processing HH:  44%|████▎     | 70268/160800 [00:05<00:06, 13777.36it/s]Processing HH:  45%|████▍     | 71691/160800 [00:05<00:06, 13909.33it/s]Processing HH:  45%|████▌     | 73106/160800 [00:05<00:06, 13975.05it/s]Processing HH:  46%|████▋     | 74536/160800 [00:05<00:06, 14071.31it/s]Processing HH:  47%|████▋     | 75952/160800 [00:05<00:06, 14097.09it/s]Processing HH:  48%|████▊     | 77366/160800 [00:05<00:06, 13806.65it/s]Processing HH:  49%|████▉     | 78751/160800 [00:05<00:05, 13784.77it/s]Processing HH:  50%|████▉     | 80201/160800 [00:05<00:05, 13992.50it/s]Processing HH:  51%|█████     | 81603/160800 [00:06<00:05, 13954.50it/s]Processing HH:  52%|█████▏    | 83050/160800 [00:06<00:05, 14106.10it/s]Processing HH:  53%|█████▎    | 84485/160800 [00:06<00:05, 14178.60it/s]Processing HH:  53%|█████▎    | 85926/160800 [00:06<00:05, 14246.73it/s]Processing HH:  54%|█████▍    | 87352/160800 [00:06<00:08, 8633.45it/s] Processing HH:  55%|█████▌    | 88485/160800 [00:06<00:08, 8470.14it/s]Processing HH:  56%|█████▌    | 89519/160800 [00:06<00:08, 8428.08it/s]Processing HH:  56%|█████▋    | 90492/160800 [00:07<00:08, 8379.28it/s]Processing HH:  57%|█████▋    | 91420/160800 [00:07<00:08, 8348.51it/s]Processing HH:  57%|█████▋    | 92318/160800 [00:07<00:08, 8441.25it/s]Processing HH:  58%|█████▊    | 93208/160800 [00:07<00:08, 8374.80it/s]Processing HH:  59%|█████▊    | 94077/160800 [00:07<00:07, 8452.51it/s]Processing HH:  59%|█████▉    | 94946/160800 [00:07<00:07, 8347.12it/s]Processing HH:  60%|█████▉    | 95797/160800 [00:07<00:07, 8198.60it/s]Processing HH:  60%|██████    | 96641/160800 [00:07<00:07, 8263.82it/s]Processing HH:  61%|██████    | 97479/160800 [00:07<00:07, 8294.58it/s]Processing HH:  61%|██████    | 98315/160800 [00:08<00:07, 8268.94it/s]Processing HH:  62%|██████▏   | 99146/160800 [00:08<00:07, 8241.78it/s]Processing HH:  62%|██████▏   | 99973/160800 [00:08<00:07, 8242.64it/s]Processing HH:  63%|██████▎   | 100800/160800 [00:08<00:07, 8202.56it/s]Processing HH:  63%|██████▎   | 101647/160800 [00:08<00:07, 8279.83it/s]Processing HH:  64%|██████▍   | 102524/160800 [00:08<00:06, 8420.76it/s]Processing HH:  64%|██████▍   | 103367/160800 [00:08<00:06, 8375.53it/s]Processing HH:  65%|██████▍   | 104207/160800 [00:08<00:06, 8379.88it/s]Processing HH:  65%|██████▌   | 105046/160800 [00:08<00:06, 8261.26it/s]Processing HH:  66%|██████▌   | 105924/160800 [00:08<00:06, 8408.72it/s]Processing HH:  66%|██████▋   | 106766/160800 [00:09<00:06, 8084.16it/s]Processing HH:  67%|██████▋   | 107578/160800 [00:09<00:06, 7982.71it/s]Processing HH:  67%|██████▋   | 108409/160800 [00:09<00:06, 8075.91it/s]Processing HH:  68%|██████▊   | 109649/160800 [00:09<00:05, 9338.38it/s]Processing HH:  69%|██████▉   | 110891/160800 [00:09<00:04, 10245.51it/s]Processing HH:  70%|██████▉   | 112130/160800 [00:09<00:04, 10878.85it/s]Processing HH:  70%|███████   | 113340/160800 [00:09<00:04, 11238.78it/s]Processing HH:  71%|███████   | 114529/160800 [00:09<00:04, 11432.24it/s]Processing HH:  72%|███████▏  | 115748/160800 [00:09<00:03, 11655.92it/s]Processing HH:  73%|███████▎  | 116991/160800 [00:09<00:03, 11880.99it/s]Processing HH:  74%|███████▎  | 118233/160800 [00:10<00:03, 12040.50it/s]Processing HH:  74%|███████▍  | 119454/160800 [00:10<00:03, 12088.92it/s]Processing HH:  75%|███████▌  | 120710/160800 [00:10<00:03, 12227.12it/s]Processing HH:  76%|███████▌  | 121941/160800 [00:10<00:03, 12251.37it/s]Processing HH:  77%|███████▋  | 123167/160800 [00:10<00:04, 7827.36it/s] Processing HH:  77%|███████▋  | 124362/160800 [00:10<00:04, 8712.03it/s]Processing HH:  78%|███████▊  | 125560/160800 [00:10<00:03, 9478.70it/s]Processing HH:  79%|███████▉  | 126840/160800 [00:10<00:03, 10312.75it/s]Processing HH:  80%|███████▉  | 128113/160800 [00:11<00:02, 10949.67it/s]Processing HH:  80%|████████  | 129420/160800 [00:11<00:02, 11530.03it/s]Processing HH:  81%|████████▏ | 130656/160800 [00:11<00:02, 11761.70it/s]Processing HH:  82%|████████▏ | 131917/160800 [00:11<00:02, 12003.18it/s]Processing HH:  83%|████████▎ | 133154/160800 [00:11<00:02, 12098.92it/s]Processing HH:  84%|████████▎ | 134409/160800 [00:11<00:02, 12230.56it/s]Processing HH:  84%|████████▍ | 135669/160800 [00:11<00:02, 12339.33it/s]Processing HH:  85%|████████▌ | 136916/160800 [00:11<00:01, 12357.78it/s]Processing HH:  86%|████████▌ | 138161/160800 [00:11<00:01, 12354.75it/s]Processing HH:  87%|████████▋ | 139405/160800 [00:11<00:01, 12375.52it/s]Processing HH:  87%|████████▋ | 140648/160800 [00:12<00:01, 12377.81it/s]Processing HH:  88%|████████▊ | 141889/160800 [00:12<00:01, 12098.76it/s]Processing HH:  89%|████████▉ | 143103/160800 [00:12<00:01, 12078.47it/s]Processing HH:  90%|████████▉ | 144385/160800 [00:12<00:01, 12295.88it/s]Processing HH:  91%|█████████ | 145651/160800 [00:12<00:01, 12403.73it/s]Processing HH:  91%|█████████▏| 146893/160800 [00:12<00:01, 12385.22it/s]Processing HH:  92%|█████████▏| 148133/160800 [00:12<00:01, 12158.15it/s]Processing HH:  93%|█████████▎| 149351/160800 [00:12<00:00, 11980.11it/s]Processing HH:  94%|█████████▎| 150554/160800 [00:12<00:00, 11992.26it/s]Processing HH:  94%|█████████▍| 151795/160800 [00:12<00:00, 12114.49it/s]Processing HH:  95%|█████████▌| 153008/160800 [00:13<00:00, 12087.57it/s]Processing HH:  96%|█████████▌| 154218/160800 [00:13<00:00, 12070.16it/s]Processing HH:  97%|█████████▋| 155457/160800 [00:13<00:00, 12161.00it/s]Processing HH:  97%|█████████▋| 156712/160800 [00:13<00:00, 12275.14it/s]Processing HH:  98%|█████████▊| 157955/160800 [00:13<00:00, 12320.30it/s]Processing HH:  99%|█████████▉| 159213/160800 [00:13<00:00, 12395.86it/s]Processing HH: 100%|█████████▉| 160453/160800 [00:13<00:00, 12365.81it/s]Processing HH: 100%|██████████| 160800/160800 [00:13<00:00, 11743.56it/s]
Loading HH dataset (test split) from Huggingface...
Processing HH:   0%|          | 0/8552 [00:00<?, ?it/s]Processing HH:  17%|█▋        | 1424/8552 [00:00<00:00, 14236.35it/s]Processing HH:  33%|███▎      | 2848/8552 [00:00<00:00, 14153.28it/s]Processing HH:  50%|████▉     | 4264/8552 [00:00<00:00, 13671.01it/s]Processing HH:  66%|██████▌   | 5633/8552 [00:00<00:00, 11615.35it/s]Processing HH:  80%|███████▉  | 6832/8552 [00:00<00:00, 11232.26it/s]Processing HH:  93%|█████████▎| 7978/8552 [00:01<00:00, 5109.09it/s] Processing HH: 100%|██████████| 8552/8552 [00:01<00:00, 7651.92it/s]
starting 4 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 1048576 from 1024
4
3 initializing distributed
Creating trainer on process 3 with world size 4
Loaded model on rank 3
4
0 initializing distributed
Creating trainer on process 0 with world size 4
Finished generating 512 examples on test split
Loaded 32 eval batches of size 16
Sharding models...
Attempting to enable activation checkpointing...
Applying activation checkpointing wrapper to policy...
FSDP activation checkpointing enabled!
Loaded model on rank 0
Using RMSprop optimizer with learning rate 1e-06
4
1 initializing distributed
Creating trainer on process 1 with world size 4
Loaded model on rank 1
4
2 initializing distributed
Creating trainer on process 2 with world size 4
Loaded model on rank 2
Error executing job with overrides: ['loss=ppo', 'model=llama_2_7b', 'datasets=[hh]', 'exp_name=llama2_7b_ppo_halos_2', 'mode=train', '++cache_dir=/home/wxt/.cache/huggingface/hub', '++trainer=PPOTrainer', '++use_fsdp=true', '++lr=1e-6', '++model.load_from=/home/wxt/.cache/huggingface/hub/llama2_7b_sft_halos_2_3/LATEST/policy.pt']
Traceback (most recent call last):
  File "/home/wxt/huatong/HALOs/train.py", line 229, in main
    mp.spawn(worker_main, nprocs=world_size, args=(world_size, config, tokenizer, train_iterator, eval_iterator, policy, reference_model), join=True)
  File "/home/wxt/.conda/envs/halos3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/wxt/.conda/envs/halos3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/home/wxt/.conda/envs/halos3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/home/wxt/.conda/envs/halos3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/home/wxt/huatong/HALOs/train.py", line 85, in worker_main
    trainer.train()
  File "/home/wxt/huatong/HALOs/trainers.py", line 1228, in train
    output=self.policy.generate(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], max_length=100)
KeyError: 'input_ids'


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
