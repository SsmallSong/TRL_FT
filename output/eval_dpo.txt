1111111111111
2222222222222222
333333333333
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.35it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.58it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.80it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.71it/s]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Traceback (most recent call last):
  File "/home/wxt/huatong/TRL_FT/test_code/eval_res.py", line 144, in <module>
    inputs = get_mess(batch_mess)
  File "/home/wxt/huatong/TRL_FT/test_code/eval_res.py", line 138, in get_mess
    mess = ray_tokenizer(mess, padding=True, max_length=4096, truncation=True,
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2859, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2945, in _call_one
    return self.batch_encode_plus(
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3136, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 537, in _batch_encode_plus
    for key in tokens_and_encodings[0][0].keys():
IndexError: list index out of range
