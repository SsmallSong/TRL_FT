# # -*- coding: utf-8 -*-
"""RAG System Using Llama2 With Hugging Face.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HsKNhtqnoH3G2wdgr8-iN0Snj7kbCJKt
"""

# !pip install pypdf

# !pip install transformers einops accelerate langchain bitsandbytes

# ## Embeddings
# !pip install sentence_transformers

# !pip install llama_index
import os
import torch
os.environ["CUDA_VISIBLE_DEVICES"]="0,1"
print(torch.cuda.device_count())
import chromadb
from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.prompts.prompts import SimpleInputPrompt
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext


documents= SimpleDirectoryReader('/home/wxt/huatong/renmin_docs').load_data()
print(type(documents))
print(len(documents))
print(documents[0])
#documents=documents[0:20]
system_prompt="""
你是一个问答助手。你的目标是根据提供的指令和上下文尽可能准确地回答问题。
你的所有回答都应该是中文的。
知识库中每篇文章都提供了url，每回答一个问题，都要在后面同时返回你是从哪个url对应的文章中找到的答案。
"""

## Default Format Supportable By Llama2
query_wrapper_prompt= SimpleInputPrompt("<USER|>{query_string}<|ASSISTANT>")

#query_wrapper_prompt

# !huggingface-cli login

modelid='mistralai/Mistral-7B-Instruct-v0.2'
modelid="itpossible/Chinese-Mistral-7B-Instruct-v0.1"
llm = HuggingFaceLLM(
    context_window=1024,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
#    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=modelid,
    model_name=modelid,
    device_map="auto",
    # uncomment this if using CUDA to reduce memory usage
    model_kwargs={"torch_dtype": torch.float16 , "load_in_8bit":True}
)



embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))

service_context=ServiceContext.from_defaults(
    chunk_size=1024,
    llm=llm,
    embed_model=embed_model
)

print("Begin Index")

db = chromadb.PersistentClient(path="/home/wxt/huatong/rmrb_chroma_db")
chroma_collection = db.get_or_create_collection("quickstart")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)



index=VectorStoreIndex.from_documents(documents,storage_context=storage_context,service_context=service_context)
#index = VectorStoreIndex.from_vector_store( vector_store,service_context=service_context, storage_context=storage_context)
#index.save_to_disk('/home/wxt/huatong/rmrb_index_file.faiss')
print("Finish Index")
query_engine=index.as_query_engine()
print("111111111111")
#response=query_engine.query("what is attention is all you need?",query_string="what is attention is all you need?")
response=query_engine.query("2024年是中国红十字会成立多少周年?")
print(response)
kill
response=query_engine.query("《中华人民共和国爱国主义教育法》什么时候实施？")
print(response)

response=query_engine.query("2024年3月18日,习近平总书记在湖南考察期间第一站来到了哪所学校？")
print(response)

response=query_engine.query("2024年我国文化和旅游部部长是谁？")
print(response)

response=query_engine.query("2023—2024赛季国际滑联短道速滑世界杯北京站比赛中，刘少昂参与获得几枚奖牌？")
print(response)

response=query_engine.query("福建自贸试验区在自贸建设十年中主要从哪几个方面推动改革创新？")
print(response)

