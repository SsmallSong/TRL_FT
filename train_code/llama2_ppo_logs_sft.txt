[2024-06-09 06:21:19,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-09 06:21:20,138] torch.distributed.run: [WARNING] 
[2024-06-09 06:21:20,138] torch.distributed.run: [WARNING] *****************************************
[2024-06-09 06:21:20,138] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-06-09 06:21:20,138] torch.distributed.run: [WARNING] *****************************************
hello, reward model
hello, reward model
hello, reward model
hello, reward model
[2024-06-09 06:21:23,370] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-09 06:21:23,441] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-09 06:21:23,442] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-09 06:21:23,456] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m On Ampere and higher architectures please use CUDA 11+
++++++++++++++++++++
come on!
++++++++++++++++++++
[2024-06-09 06:21:23,976] [INFO] [comm.py:637:init_distributed] cdb=None
++++++++++++++++++++
come on!
++++++++++++++++++++
[2024-06-09 06:21:23,988] [INFO] [comm.py:637:init_distributed] cdb=None
++++++++++++++++++++
come on!
++++++++++++++++++++
[2024-06-09 06:21:24,018] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-09 06:21:24,018] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
++++++++++++++++++++
come on!
++++++++++++++++++++
[2024-06-09 06:21:24,029] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-09 06:21:25,630] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.15s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.03s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.92s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.86s/it]
[2024-06-09 06:21:31,539] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 13.48B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.46s/it]
[2024-06-09 06:21:37,158] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 778, num_elems = 14.39B
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2024-06-09 06:21:39,491] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 974, num_elems = 15.29B
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at EleutherAI/pythia-1b-deduped and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map (num_proc=4):   0%|          | 0/160800 [00:00<?, ? examples/s]Map (num_proc=4):   1%|          | 2000/160800 [00:00<00:14, 10895.43 examples/s]Map (num_proc=4):   0%|          | 0/160800 [00:00<?, ? examples/s]Map (num_proc=4):   8%|▊         | 13000/160800 [00:00<00:02, 54006.14 examples/s]Map (num_proc=4):   1%|          | 1000/160800 [00:00<00:27, 5780.25 examples/s]Map (num_proc=4):  14%|█▍        | 23000/160800 [00:00<00:02, 68893.01 examples/s]Map (num_proc=4):   0%|          | 0/160800 [00:00<?, ? examples/s]Map (num_proc=4):   6%|▌         | 10000/160800 [00:00<00:03, 37920.08 examples/s]Map (num_proc=4):  23%|██▎       | 37000/160800 [00:00<00:01, 84057.61 examples/s]Map (num_proc=4):  14%|█▎        | 22000/160800 [00:00<00:02, 66986.77 examples/s]Map (num_proc=4):  32%|███▏      | 51000/160800 [00:00<00:01, 94260.33 examples/s]Map (num_proc=4):   1%|          | 1000/160800 [00:00<00:27, 5785.50 examples/s]Map (num_proc=4):  21%|██        | 33000/160800 [00:00<00:01, 72755.12 examples/s]Map (num_proc=4):   0%|          | 0/160800 [00:00<?, ? examples/s]Map (num_proc=4):   7%|▋         | 12000/160800 [00:00<00:02, 52230.39 examples/s]Map (num_proc=4):  40%|████      | 65000/160800 [00:00<00:00, 102563.05 examples/s]Map (num_proc=4):  28%|██▊       | 45000/160800 [00:00<00:01, 81634.45 examples/s]Map (num_proc=4):  48%|████▊     | 77000/160800 [00:00<00:00, 105398.62 examples/s]Map (num_proc=4):  14%|█▎        | 22000/160800 [00:00<00:02, 64462.39 examples/s]Map (num_proc=4):   1%|          | 1000/160800 [00:00<00:31, 5084.69 examples/s]Map (num_proc=4):  35%|███▍      | 56000/160800 [00:00<00:01, 86745.14 examples/s]Map (num_proc=4):  57%|█████▋    | 91000/160800 [00:00<00:00, 110422.78 examples/s]Map (num_proc=4):  22%|██▏       | 36000/160800 [00:00<00:01, 79759.90 examples/s]Map (num_proc=4):   7%|▋         | 12000/160800 [00:00<00:03, 46494.83 examples/s]Map (num_proc=4):  41%|████      | 66000/160800 [00:00<00:01, 85513.12 examples/s]Map (num_proc=4):  65%|██████▍   | 104000/160800 [00:01<00:00, 114668.35 examples/s]Map (num_proc=4):  30%|██▉       | 48000/160800 [00:00<00:01, 83217.87 examples/s]Map (num_proc=4):  15%|█▍        | 24000/160800 [00:00<00:01, 68892.62 examples/s]Map (num_proc=4):  49%|████▉     | 79000/160800 [00:01<00:00, 97265.16 examples/s]Map (num_proc=4):  21%|██        | 33000/160800 [00:00<00:01, 75020.13 examples/s]Map (num_proc=4):  39%|███▉      | 63000/160800 [00:00<00:01, 90264.51 examples/s]Map (num_proc=4):  56%|█████▌    | 90000/160800 [00:01<00:00, 93118.16 examples/s]Map (num_proc=4):  72%|███████▏  | 116000/160800 [00:01<00:00, 85672.45 examples/s] Map (num_proc=4):  27%|██▋       | 44000/160800 [00:00<00:01, 84526.17 examples/s]Map (num_proc=4):  63%|██████▎   | 102000/160800 [00:01<00:00, 97542.24 examples/s]Map (num_proc=4):  48%|████▊     | 77000/160800 [00:00<00:00, 94829.92 examples/s]Map (num_proc=4):  33%|███▎      | 53000/160800 [00:00<00:01, 85186.36 examples/s]Map (num_proc=4):  78%|███████▊  | 126000/160800 [00:01<00:00, 77672.78 examples/s]Map (num_proc=4):  70%|██████▉   | 112000/160800 [00:01<00:00, 94983.30 examples/s]Map (num_proc=4):  57%|█████▋    | 91000/160800 [00:01<00:00, 99562.99 examples/s]Map (num_proc=4):  86%|████████▌ | 138200/160800 [00:01<00:00, 87436.99 examples/s]Map (num_proc=4):  40%|████      | 65000/160800 [00:00<00:01, 94913.02 examples/s]Map (num_proc=4):  64%|██████▍   | 103000/160800 [00:01<00:00, 100525.31 examples/s]Map (num_proc=4):  47%|████▋     | 76000/160800 [00:00<00:00, 94143.81 examples/s]Map (num_proc=4):  72%|███████▏  | 115000/160800 [00:01<00:00, 96865.15 examples/s] Map (num_proc=4):  92%|█████████▏| 148600/160800 [00:01<00:00, 68836.25 examples/s]Map (num_proc=4):  54%|█████▍    | 87000/160800 [00:01<00:00, 94593.90 examples/s]Map (num_proc=4):  62%|██████▏   | 99000/160800 [00:01<00:00, 95395.72 examples/s]Map (num_proc=4):  76%|███████▌  | 122000/160800 [00:01<00:00, 47824.85 examples/s]Map (num_proc=4):  69%|██████▉   | 111000/160800 [00:01<00:00, 92429.77 examples/s]Map (num_proc=4):  81%|████████▏ | 131000/160800 [00:01<00:00, 49652.36 examples/s]Map (num_proc=4):  78%|███████▊  | 125000/160800 [00:01<00:00, 54163.58 examples/s]Map (num_proc=4):  88%|████████▊ | 142000/160800 [00:02<00:00, 58440.94 examples/s]Map (num_proc=4):  76%|███████▌  | 122000/160800 [00:01<00:00, 73819.44 examples/s]Map (num_proc=4):  83%|████████▎ | 134200/160800 [00:01<00:00, 58379.33 examples/s]Map (num_proc=4):  82%|████████▏ | 131200/160800 [00:01<00:00, 71798.08 examples/s]Map (num_proc=4):  88%|████████▊ | 142200/160800 [00:01<00:00, 61031.87 examples/s]Map (num_proc=4):  95%|█████████▍| 152600/160800 [00:02<00:00, 58528.30 examples/s]Map (num_proc=4):  95%|█████████▍| 152400/160800 [00:02<00:00, 69221.57 examples/s]Map (num_proc=4):  87%|████████▋ | 140200/160800 [00:01<00:00, 70597.93 examples/s]Map (num_proc=4):  99%|█████████▊| 158600/160800 [00:02<00:00, 31279.10 examples/s]Map (num_proc=4):  92%|█████████▏| 147600/160800 [00:01<00:00, 62500.75 examples/s]Map (num_proc=4): 100%|██████████| 160800/160800 [00:02<00:00, 58270.82 examples/s]
Map (num_proc=4):   0%|          | 0/8552 [00:00<?, ? examples/s]Map (num_proc=4):  23%|██▎       | 2000/8552 [00:00<00:00, 11346.46 examples/s]Map (num_proc=4): 100%|█████████▉| 160600/160800 [00:02<00:00, 32432.07 examples/s]Map (num_proc=4): 100%|█████████▉| 160600/160800 [00:02<00:00, 36461.01 examples/s]Map (num_proc=4): 100%|██████████| 160800/160800 [00:02<00:00, 54434.32 examples/s]
Map (num_proc=4):  98%|█████████▊| 8414/8552 [00:00<00:00, 32160.68 examples/s]Map (num_proc=4): 100%|██████████| 160800/160800 [00:02<00:00, 59455.80 examples/s]
Map (num_proc=4):  96%|█████████▌| 154600/160800 [00:02<00:00, 33832.98 examples/s]Map (num_proc=4): 100%|██████████| 8552/8552 [00:00<00:00, 22696.37 examples/s]
Map (num_proc=4):   0%|          | 0/8552 [00:00<?, ? examples/s]Map (num_proc=4):   0%|          | 0/8552 [00:00<?, ? examples/s]Map (num_proc=4):  12%|█▏        | 1000/8552 [00:00<00:01, 6317.31 examples/s]Map (num_proc=4): 100%|█████████▉| 160600/160800 [00:02<00:00, 30891.27 examples/s]Map (num_proc=4):  23%|██▎       | 2000/8552 [00:00<00:00, 10323.96 examples/s]Map (num_proc=4):  87%|████████▋ | 7414/8552 [00:00<00:00, 29531.75 examples/s]Map (num_proc=4): 100%|██████████| 160800/160800 [00:02<00:00, 56806.11 examples/s]
Map (num_proc=4):  98%|█████████▊| 8414/8552 [00:00<00:00, 31921.86 examples/s]Map (num_proc=4):   0%|          | 0/8552 [00:00<?, ? examples/s]Map (num_proc=4): 100%|██████████| 8552/8552 [00:00<00:00, 22170.95 examples/s]
Map (num_proc=4): 100%|██████████| 8552/8552 [00:00<00:00, 19834.34 examples/s]
Map (num_proc=4):  23%|██▎       | 2000/8552 [00:00<00:00, 10232.37 examples/s][2024-06-09 06:21:55,722] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown
[2024-06-09 06:21:55,738] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Map (num_proc=4):  98%|█████████▊| 8414/8552 [00:00<00:00, 30894.44 examples/s][2024-06-09 06:21:55,739] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-09 06:21:55,740] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-09 06:21:55,762] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-09 06:21:55,762] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-09 06:21:55,762] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-06-09 06:21:55,762] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
Map (num_proc=4): 100%|██████████| 8552/8552 [00:00<00:00, 21809.80 examples/s]
[2024-06-09 06:21:55,976] [INFO] [utils.py:779:see_memory_usage] Stage 3 initialize beginning
[2024-06-09 06:21:55,977] [INFO] [utils.py:780:see_memory_usage] MA 7.29 GB         Max_MA 7.69 GB         CA 7.91 GB         Max_CA 8 GB 
[2024-06-09 06:21:55,977] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.23 GB, percent = 2.8%
[2024-06-09 06:21:55,980] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-06-09 06:21:55,980] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-06-09 06:21:56,205] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-06-09 06:21:56,206] [INFO] [utils.py:780:see_memory_usage] MA 7.29 GB         Max_MA 7.29 GB         CA 7.91 GB         Max_CA 8 GB 
[2024-06-09 06:21:56,206] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.25 GB, percent = 2.8%
Parameter Offload: Total persistent parameters: 698368 in 196 params
[2024-06-09 06:21:56,421] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-06-09 06:21:56,422] [INFO] [utils.py:780:see_memory_usage] MA 7.29 GB         Max_MA 7.29 GB         CA 7.91 GB         Max_CA 8 GB 
[2024-06-09 06:21:56,422] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.26 GB, percent = 2.8%
[2024-06-09 06:21:56,585] [INFO] [utils.py:779:see_memory_usage] Before creating fp16 partitions
[2024-06-09 06:21:56,586] [INFO] [utils.py:780:see_memory_usage] MA 7.29 GB         Max_MA 7.29 GB         CA 7.91 GB         Max_CA 8 GB 
[2024-06-09 06:21:56,586] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.26 GB, percent = 2.8%
[2024-06-09 06:21:57,775] [INFO] [utils.py:779:see_memory_usage] After creating fp16 partitions: 3
[2024-06-09 06:21:57,776] [INFO] [utils.py:780:see_memory_usage] MA 7.28 GB         Max_MA 7.29 GB         CA 8.69 GB         Max_CA 9 GB 
[2024-06-09 06:21:57,776] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 31.41 GB, percent = 3.6%
[2024-06-09 06:21:57,936] [INFO] [utils.py:779:see_memory_usage] Before creating fp32 partitions
[2024-06-09 06:21:57,936] [INFO] [utils.py:780:see_memory_usage] MA 7.28 GB         Max_MA 7.28 GB         CA 8.69 GB         Max_CA 9 GB 
[2024-06-09 06:21:57,937] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 27.85 GB, percent = 3.2%
[2024-06-09 06:21:58,094] [INFO] [utils.py:779:see_memory_usage] After creating fp32 partitions
[2024-06-09 06:21:58,094] [INFO] [utils.py:780:see_memory_usage] MA 14.4 GB         Max_MA 16.1 GB         CA 17.68 GB         Max_CA 18 GB 
[2024-06-09 06:21:58,095] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.28 GB, percent = 2.8%
[2024-06-09 06:21:58,247] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states
[2024-06-09 06:21:58,248] [INFO] [utils.py:780:see_memory_usage] MA 14.4 GB         Max_MA 14.4 GB         CA 17.68 GB         Max_CA 18 GB 
[2024-06-09 06:21:58,248] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.28 GB, percent = 2.8%
[2024-06-09 06:21:58,403] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states
[2024-06-09 06:21:58,403] [INFO] [utils.py:780:see_memory_usage] MA 14.4 GB         Max_MA 18.14 GB         CA 21.42 GB         Max_CA 21 GB 
[2024-06-09 06:21:58,403] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.28 GB, percent = 2.8%
[2024-06-09 06:21:58,404] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-06-09 06:21:59,227] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer
[2024-06-09 06:21:59,228] [INFO] [utils.py:780:see_memory_usage] MA 18.9 GB         Max_MA 19.39 GB         CA 21.42 GB         Max_CA 21 GB 
[2024-06-09 06:21:59,228] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.32 GB, percent = 2.8%
[2024-06-09 06:21:59,229] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-06-09 06:21:59,229] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-09 06:21:59,229] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-09 06:21:59,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-06, 3e-06], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-06-09 06:21:59,230] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8641fa38e0>
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-06-09 06:21:59,231] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-06-09 06:21:59,232] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   train_batch_size ............. 512
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-09 06:21:59,233] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-06-09 06:21:59,233] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 8, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[2024-06-09 06:21:59,234] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown
[2024-06-09 06:21:59,240] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-09 06:21:59,241] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
[2024-06-09 06:21:59,412] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-06-09 06:21:59,413] [INFO] [utils.py:780:see_memory_usage] MA 18.89 GB         Max_MA 18.9 GB         CA 21.42 GB         Max_CA 21 GB 
[2024-06-09 06:21:59,413] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.32 GB, percent = 2.8%
Parameter Offload: Total persistent parameters: 432128 in 131 params
[2024-06-09 06:21:59,579] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-06-09 06:21:59,579] [INFO] [utils.py:780:see_memory_usage] MA 18.89 GB         Max_MA 18.89 GB         CA 21.42 GB         Max_CA 21 GB 
[2024-06-09 06:21:59,580] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.32 GB, percent = 2.8%
[2024-06-09 06:21:59,580] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-06-09 06:21:59,580] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-09 06:21:59,580] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-09 06:21:59,580] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-06-09 06:21:59,580] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8641a40460>
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-06-09 06:21:59,581] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   train_batch_size ............. 512
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-06-09 06:21:59,582] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-06-09 06:21:59,583] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-09 06:21:59,583] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-06-09 06:21:59,583] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-09 06:21:59,583] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-06-09 06:21:59,583] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 8, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "zero_optimization.reduce_bucket_size": 4.194304e+06, 
    "zero_optimization.stage3_param_persistence_threshold": 2.048000e+04, 
    "zero_optimization.stage3_prefetch_bucket_size": 0
}
[2024-06-09 06:21:59,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown
[2024-06-09 06:21:59,592] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-09 06:21:59,593] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
[2024-06-09 06:21:59,734] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-06-09 06:21:59,735] [INFO] [utils.py:780:see_memory_usage] MA 18.89 GB         Max_MA 18.89 GB         CA 21.42 GB         Max_CA 21 GB 
[2024-06-09 06:21:59,735] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.32 GB, percent = 2.8%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-06-09 06:21:59,896] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-06-09 06:21:59,897] [INFO] [utils.py:780:see_memory_usage] MA 18.89 GB         Max_MA 18.89 GB         CA 21.42 GB         Max_CA 21 GB 
[2024-06-09 06:21:59,897] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 24.32 GB, percent = 2.8%
[2024-06-09 06:21:59,898] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8641920a30>
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-06-09 06:21:59,898] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 8
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-06-09 06:21:59,899] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   train_batch_size ............. 512
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-09 06:21:59,900] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-06-09 06:21:59,900] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_accumulation_steps": 8, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "zero_optimization.reduce_bucket_size": 1.677722e+07, 
    "zero_optimization.stage3_param_persistence_threshold": 4.096000e+04, 
    "zero_optimization.stage3_prefetch_bucket_size": 0
}
===training policy===
wandb: Currently logged in as: 3151273556 (run_sht). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /home/wxt/huatong/TRL_FT/train_code/wandb/run-20240609_062315-5152i3r5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ppov2_config__42__1717914115
wandb: ⭐️ View project at https://wandb.ai/run_sht/huggingface
wandb: 🚀 View run at https://wandb.ai/run_sht/huggingface/runs/5152i3r5
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ query                                                ┃ model response                                                                                                                                                                                                                                ┃ score           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ what are some pranks with a pen i can do?            │ </s><s> Home > Blog > What is the Difference Between a Will and a Trust?                                                                                                                                                                      │ -0.03857421875  │
│                                                      │ What is the Difference Between a Will and a Trust?                                                                                                                                                                                            │                 │
│                                                      │ A will is a legal document that specifies your wishes on how to deal with your property after you                                                                                                                                             │                 │
├──────────────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────┤
│ How much alcohol can I drink per day?                │                                                                                                                                                                                                                                               │ -0.91015625     │
│                                                      │  Research suggests that moderate alcohol consumption is defined as 1-2 drinks per day for men, and no more than 1 drink per day for women and people who are not overweight. However, it’s important to remember that these are               │                 │
├──────────────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────┤
│ what are some pranks i can play on a nerd at school? │                                                                                                                                                                                                                                               │ -0.94140625     │
│                                                      │  Tags: pranks, school, nerd                                                                                                                                                                                                                   │                 │
│                                                      │ What are some pranks I can play on a nerd at school?</s><s> Home > News & Reviews > The National                                                                                                                                              │                 │
│                                                      │ The National Share New Single, 'Weird'                                                                                                                                                                                                        │                 │
│                                                      │ The National have shared                                                                                                                                                                                                                      │                 │
├──────────────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────┤
│ How do I pick a lock?                                │                                                                                                                                                                                                                                               │ 1.4375          │
│                                                      │ Lock picking is a skill that requires practice and patience. There are many different types of locks and locking mechanisms, and each requires a different technique. The best way to learn how to pick a lock is to practice on old locks    │                 │
│                                                      │ that                                                                                                                                                                                                                                          │                 │
├──────────────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────┤
│ What will happen if I drive my car into the water?   │                                                                                                                                                                                                                                               │ -0.060302734375 │
│                                                      │ Will my car float?                                                                                                                                                                                                                            │                 │
│                                                      │ Will it sink?                                                                                                                                                                                                                                 │                 │
│                                                      │ Will it be damaged?                                                                                                                                                                                                                           │                 │
│                                                      │ Will it be destroyed?                                                                                                                                                                                                                         │                 │
│                                                      │ What will happen if I drive my car into the water?                                                                                                                                                                                            │                 │
│                                                      │ Will my car float?                                                                                                                                                                                                                            │                 │
│                                                      │ Will it sink?                                                                                                                                                                                                                                 │                 │
│                                                      │ Will it be dam                                                                                                                                                                                                                                │                 │
└──────────────────────────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────┘
