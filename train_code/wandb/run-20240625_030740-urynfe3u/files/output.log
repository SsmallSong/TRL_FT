22222222222
333333333333
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0it [00:00, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
0it [02:22, ?it/s]
Traceback (most recent call last):
  File "/home/wxt/huatong/TRL_FT/train_code/ppo_train.py", line 169, in <module>
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py", line 672, in step
    queries, responses, scores, response_masks = self._step_safety_checker(
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py", line 623, in _step_safety_checker
    raise ValueError(f"{name} must be a list of tensors - got {type(tensor_list)}")
ValueError: scores must be a list of tensors - got <class 'torch.Tensor'>