
[2024-06-25 05:17:05,876] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-06-25 05:17:10,062] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-25 05:17:10,064] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-25 05:17:10,064] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-25 05:17:10,074] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-06-25 05:17:10,074] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2024-06-25 05:17:10,075] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-06-25 05:17:10,075] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-06-25 05:17:10,216] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2024-06-25 05:17:10,217] [INFO] [utils.py:782:see_memory_usage] MA 12.55 GB         Max_MA 12.55 GB         CA 12.55 GB         Max_CA 13 GB
[2024-06-25 05:17:10,217] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.13 GB, percent = 14.1%
[2024-06-25 05:17:10,218] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-06-25 05:17:10,219] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-06-25 05:17:10,338] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-06-25 05:17:10,339] [INFO] [utils.py:782:see_memory_usage] MA 12.55 GB         Max_MA 12.55 GB         CA 12.55 GB         Max_CA 13 GB
[2024-06-25 05:17:10,340] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.13 GB, percent = 14.1%
Parameter Offload: Total persistent parameters: 270337 in 67 params
[2024-06-25 05:17:11,140] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-06-25 05:17:11,141] [INFO] [utils.py:782:see_memory_usage] MA 3.14 GB         Max_MA 12.61 GB         CA 12.62 GB         Max_CA 13 GB
[2024-06-25 05:17:11,141] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.35 GB, percent = 14.1%
[2024-06-25 05:17:11,623] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2024-06-25 05:17:11,624] [INFO] [utils.py:782:see_memory_usage] MA 3.14 GB         Max_MA 3.14 GB         CA 12.62 GB         Max_CA 13 GB
[2024-06-25 05:17:11,625] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.36 GB, percent = 14.1%
[2024-06-25 05:17:13,169] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[2024-06-25 05:17:13,170] [INFO] [utils.py:782:see_memory_usage] MA 3.14 GB         Max_MA 3.14 GB         CA 3.14 GB         Max_CA 13 GB
[2024-06-25 05:17:13,170] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.37 GB, percent = 14.1%
[2024-06-25 05:17:13,295] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2024-06-25 05:17:13,296] [INFO] [utils.py:782:see_memory_usage] MA 3.14 GB         Max_MA 3.14 GB         CA 3.14 GB         Max_CA 3 GB
[2024-06-25 05:17:13,296] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.37 GB, percent = 14.1%
[2024-06-25 05:17:13,434] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2024-06-25 05:17:13,435] [INFO] [utils.py:782:see_memory_usage] MA 9.41 GB         Max_MA 10.68 GB         CA 11.29 GB         Max_CA 11 GB
[2024-06-25 05:17:13,435] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.38 GB, percent = 14.1%
[2024-06-25 05:17:13,559] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-06-25 05:17:13,560] [INFO] [utils.py:782:see_memory_usage] MA 9.41 GB         Max_MA 9.41 GB         CA 11.29 GB         Max_CA 11 GB
[2024-06-25 05:17:13,560] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.38 GB, percent = 14.1%
[2024-06-25 05:17:13,681] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-06-25 05:17:13,682] [INFO] [utils.py:782:see_memory_usage] MA 9.41 GB         Max_MA 13.15 GB         CA 15.03 GB         Max_CA 15 GB
[2024-06-25 05:17:13,682] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 122.38 GB, percent = 14.1%
[2024-06-25 05:17:13,683] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-06-25 05:17:14,002] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-06-25 05:17:14,003] [INFO] [utils.py:782:see_memory_usage] MA 13.48 GB         Max_MA 13.97 GB         CA 15.03 GB         Max_CA 15 GB
[2024-06-25 05:17:14,003] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 120.4 GB, percent = 13.9%
[2024-06-25 05:17:14,003] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2024-06-25 05:17:14,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-25 05:17:14,004] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-25 05:17:14,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-06], mom=[(0.9, 0.999)]
[2024-06-25 05:17:14,005] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-06-25 05:17:14,005] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2024-06-25 05:17:14,005] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-25 05:17:14,005] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2efd1af640>
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-25 05:17:14,006] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-06-25 05:17:14,007] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 8
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-06-25 05:17:14,008] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-06-25 05:17:14,009] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   train_batch_size ............. 4096
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  128
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   world_size ................... 4
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-25 05:17:14,010] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 3
[2024-06-25 05:17:14,011] [INFO] [config.py:987:print_user_config]   json = {
    "train_batch_size": 4.096000e+03,
    "train_micro_batch_size_per_gpu": 128,
    "gradient_accumulation_steps": 8,
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "none",
            "nvme_path": null
        },
        "offload_param": {
            "device": "none",
            "nvme_path": null
        },
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "gradient_clipping": 1.0,
    "steps_per_print": inf,
    "bf16": {
        "enabled": true
    },
    "fp16": {
        "enabled": false
    },
    "zero_allow_untested_optimizer": true
}
[2024-06-25 05:17:14,011] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-06-25 05:17:18,383] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-25 05:17:18,385] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
[2024-06-25 05:17:18,558] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-06-25 05:17:18,559] [INFO] [utils.py:782:see_memory_usage] MA 26.03 GB         Max_MA 26.03 GB         CA 26.08 GB         Max_CA 26 GB
[2024-06-25 05:17:18,559] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.14 GB, percent = 2.6%
Parameter Offload: Total persistent parameters: 270337 in 67 params
[2024-06-25 05:17:18,813] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-06-25 05:17:18,814] [INFO] [utils.py:782:see_memory_usage] MA 16.62 GB         Max_MA 26.1 GB         CA 26.14 GB         Max_CA 26 GB
[2024-06-25 05:17:18,814] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.53 GB, percent = 2.6%
[2024-06-25 05:17:18,815] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-06-25 05:17:18,816] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2efd1af9a0>
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-25 05:17:18,817] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 8
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-06-25 05:17:18,818] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-06-25 05:17:18,819] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   train_batch_size ............. 4096
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  128
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-06-25 05:17:18,820] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   world_size ................... 4
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-25 05:17:18,821] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 3
[2024-06-25 05:17:18,821] [INFO] [config.py:987:print_user_config]   json = {
    "train_batch_size": 4.096000e+03,
    "train_micro_batch_size_per_gpu": 128,
    "gradient_accumulation_steps": 8,
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "none",
            "nvme_path": null
        },
        "offload_param": {
            "device": "none",
            "nvme_path": null
        },
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "gradient_clipping": 1.0,
    "steps_per_print": inf,
    "bf16": {
        "enabled": true
    },
    "fp16": {
        "enabled": false
    },
    "zero_allow_untested_optimizer": true,
    "zero_optimization.reduce_bucket_size": 1.677722e+07,
    "zero_optimization.stage3_param_persistence_threshold": 4.096000e+04,
    "zero_optimization.stage3_prefetch_bucket_size": 1.509949e+07
}
22222222222
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-06-25 05:17:20,394] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 293, num_elems = 1.31B
0it [00:00, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
333333333333
444444444444
0it [05:56, ?it/s]
Traceback (most recent call last):
  File "/home/wxt/huatong/TRL_FT/train_code/ppo_train.py", line 171, in <module>
    rewards_tensor = rm(**inputs).logits#.cpu().detach()
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxt/huatong/TRL_FT/train_code/model_training/models/reward_model.py", line 90, in forward
    outputs = self.gpt_neox(
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py", line 892, in forward
    inputs_embeds = self.embed_in(input_ids)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/modules/sparse.py", line 163, in forward
    return F.embedding(
  File "/home/wxt/.conda/envs/rl/lib/python3.10/site-packages/torch/nn/functional.py", line 2237, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: 'weight' must be 2-D
55555555555555
6666666666666
====================text==========================
<|prompter|>What are some cool cartoons from the 1980s?<|endoftext|><|assistant|>I’m sorry, I’m not familiar with any cartoons from the 1980s. Could you be more specific?</s><|endoftext|>
<|prompter|>I went hunting for the first time and shot a buck deer. I need to skin it for the meat, but I don't know how. What are my first steps?<|endoftext|><|assistant|>I'm sorry to hear that you're inexperienced with field dressing deer. Fortunately, it's a very straightforward process. You begin by removing the skin from the rear legs, and then from the belly. Then, it's helpful to cut through the skin along the spine using a sharp knife. With the skin removed, you can then proceed to remove the internal organs, which is done using your hands. Simply grab the deer's organs and pull them out from the cavity. Finally, you're ready to remove the meat. You'<|endoftext|>
<|prompter|>Are there any countries with no speed limits on their roadways?<|endoftext|><|assistant|>Yes, there are many countries with no speed limits on their roadways. Most notably in the United States there are two types of roads that have no speed limit: Interstate highways and State highways. Many countries around the world also have sections of road with no posted speed limit. This includes certain highways in Australia, Canada, China, India, Italy, Japan, South Korea, New Zealand, and the United Kingdom. These no-speed-limit roadways are typically found in less populated areas or in areas with minimal traffic. Generally speaking, these roads are only found in rural or sparsely-populated<|endoftext|>
<|prompter|>How do I teach a child to use the scissors?<|endoftext|><|assistant|>Let’s try breaking down what you’re trying to teach. First you need to show the child that cutting a piece of paper with scissors can be done by pressing a button on the scissors and moving the scissors along with the paper. It will take a while to master, but you can explain that with a little practice it can be done. Then you need to explain how to press the button while moving the scissors. It will take a while to master, but you can explain that with a little practice it can be done. Then you need to teach the child to cut along an edge of<|endoftext|>
<|prompter|>Why is Lord of the Rings so popular in fantasy literature?<|endoftext|><|assistant|>Well, the story and characters have a lot of depth and richness, and readers love being immersed in that world. And of course the movies have been huge blockbusters for the last decade. In addition, the books are a great example of how fantasy literature can be a powerful vehicle for exploring deep themes like the nature of power and evil, and the need for courage and hope in a dark world.</s><|endoftext|>
<|prompter|>I need to create a new business email but am unsure about which service to use. Do you know which email services are best for business?<|endoftext|><|assistant|>Sure, that’s a great question. Let me explain some differences in email services to help you choose between them. For example, Gmail is a popular email service, but it’s a consumer-facing service. That means that it has most of the features of an email service, but lacks the more advanced business-oriented features like administration controls and multiple-user accounts. Another option is Microsoft Exchange, which is a full business email service, but it’s expensive. We can explore further if you’d like. There are many other options too, like Google Apps, Zoho, IBM<|endoftext|>
<|prompter|>my baby put something small in his mouth. how do i get it out?<|endoftext|><|assistant|>The procedure you're looking for is known as “foreign body removal.” The most common thing people put in their mouths is their own fingers, but you can also get other smaller objects like pebbles, beads, seeds, or pieces of jewelry. What you need to do is to take a warm, moist cloth and gently press it onto the outside of the baby's mouth while you attempt to pry open his mouth with your finger or a pair of tweezers. You may need to than tap on the back of the baby's gums a little bit with the twe<|endoftext|>
<|prompter|>How do I get my child to get faster at addition facts?<|endoftext|><|assistant|>Let me explain how the human brain works. The part of your brain that governs addition facts is like an old computer: It has a memory, the memory can be trained, and the processor (the part that does the math) is physically limited in how fast it can calculate. You’ve also got a bunch of new technologies built on top of the old computer, like the internet, or like a GPS in your car. The thing is, your brain was designed to do things like addition facts, and for that, it’s very good. It’s also designed to learn. It can be trained and<|endoftext|>
<|prompter|>What are the best crime drama shows?<|endoftext|><|assistant|>This is an especially tricky question for me, since there are so many subgenres, subtypes, and regional variants of crime drama, and they’re mostly focused on TV shows rather than films. One of the best-known crime drama series is Colombo, which has been considered a classic of the genre since the 1970s.
If you like films, the French film noir movement in the 1940s is a highly influential style of crime drama. The American crime drama film genre is a broad one, but one notable example is the “whodunit” film<|endoftext|>
<|prompter|>How do I get my boyfriend to propose?<|endoftext|><|assistant|>You can encourage him to talk about the future and how he sees your relationship progressing, by using neutral, un-threatening phrases like “I can’t wait to see what the future holds for us,” or “It feels like we’re at a point where we should be talking about a next step.” It might also help to give him a specific number of months or years until you think it will be a good time to have a conversation about marriage. Of course, you’ll want to avoid any direct or indirect pressure, and you should absolutely avoid asking him when he’ll propose! Instead,<|endoftext|>
====================text==========================